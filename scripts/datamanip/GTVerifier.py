import sys
import re
import argparse
from sklearn.metrics import pairwise_distances
import parse_common as pc


#This is a very lame implementation of GT computation so that we avoid errors
#and can compare the GTs generated by more sophisticated methods.
#This code assumes that all the data fits in memory

K = 100
STR_NO_FILTER = "__NONE__"
dist_fn_sklearn_metric_map = {"l2" : "euclidean", 
                              "cosine" : "cosine", 
                              "mips" : "linear"}


ground_truth = {}

class FilteredVectors:
    def __init__(self, filter):
        self.data = []
        self.filter = filter
        self.orig_id_fil_id_map = {}
        self.count = 0
        self.data_mat = None

    def add_vector(self, vector, orig_id):
        self.data.append(vector)
        self.orig_id_fil_id_map[self.count] = orig_id
        self.count += 1

    def assign_data_mat(self, data_mat):
        self.data_mat = data_mat

    def __len__(self):
        return len(self.data) if self.data_mat is None else self.data_mat.num_rows

    def __getitem__(self, index):
        return self.data[index]
    
    def __str__(self) -> str:
        return self.filter + " filters out " + str(self.count) + " vectors"


def parse_filter_line(line, delim_regex, line_number, filter_file_name):
    line = line.strip()
    filters = re.split(delim_regex, line)
    if len(filters) == 0:
        raise Exception(f"Filter line: {line} at line number: {line_number} in file {filter_file_name} does not have any filters")
    return filters

def process_filters(filter_file, data_mat, is_query):
    filters = []
    filtered_vectors = {}
    if filter_file is not None:
        with open(filter_file, mode='r', encoding='UTF-8') as f:
            line_num = 0
            for line in f:
                filters_of_point = parse_filter_line(line, ',', line_num+1, filter_file)
                filters.append(filters_of_point)
                for filter in filters_of_point:
                    if filter not in filtered_vectors:
                        filtered_vectors[filter] = FilteredVectors(filter)
                    filtered_vectors[filter].add_vector(data_mat.get_vector(line_num), line_num)
                line_num += 1
    else:
        if not is_query:
            filtered_vectors[STR_NO_FILTER] = FilteredVectors(STR_NO_FILTER)
            filtered_vectors[STR_NO_FILTER].assign_data_mat(data_mat) 
        else:
            #to simplify the code, we copy the queries one-by-one into the data list.
            filtered_vectors[STR_NO_FILTER] = FilteredVectors(STR_NO_FILTER)
            for i in range(data_mat.num_rows):
                filtered_vectors[STR_NO_FILTER].add_vector(data_mat.get_vector(i), i)
        
    for filter in filtered_vectors:
        print(filtered_vectors[filter])

    unique_vector_ids = set()
    for filtered_vector in filtered_vectors.values():
        unique_vector_ids.update(filtered_vector.orig_id_fil_id_map.values())
    all_ids = set(range(data_mat.num_rows))
    if len(all_ids.difference(unique_vector_ids)) > 0:
        raise Exception(f"Missing vectors in filters: {all_ids.difference(unique_vector_ids)}")

    return filters, filtered_vectors

def compute_filtered_gt(base_filtered_vector, query_filtered_vector, dist_fn):
    print(f"Computing GT for filter: {query_filtered_vector.filter}, base count: {len(base_filtered_vector)}, query count: {len(query_filtered_vector)}")
    for fil_q_id, query_vector in enumerate(query_filtered_vector.data):
        qv = query_vector.reshape(1, -1)
        if base_filtered_vector.data_mat is not None:
            dist = pairwise_distances(base_filtered_vector.data_mat.data, qv, metric=dist_fn_sklearn_metric_map[dist_fn])
        else:
            dist = pairwise_distances(base_filtered_vector.data, qv, metric=dist_fn_sklearn_metric_map[dist_fn])
        
        index_dist_pairs = [(i, dist[i][0]) for i in range(len(dist))]
        index_dist_pairs.sort(key=lambda x: x[1])
        k = min(K, len(index_dist_pairs))
        top_k_matches = index_dist_pairs[:k]
        orig_query_id = query_filtered_vector.orig_id_fil_id_map[fil_q_id]
        ground_truth[orig_query_id] = []
        for match in top_k_matches:
            ground_truth[orig_query_id].append((base_filtered_vector.orig_id_fil_id_map[match[0]], match[1]))

def compute_gt(base_filtered_vectors, query_filtered_vectors, dist_fn):
    for query_filter in query_filtered_vectors.keys():
        if query_filter not in base_filtered_vectors:
            print(f"Filter: {query_filter} in query does not exist in base")
            continue
        base_filtered_vector = base_filtered_vectors[query_filter]
        query_filtered_vector = query_filtered_vectors[query_filter]
        compute_filtered_gt(base_filtered_vector, query_filtered_vector, dist_fn)

        print(ground_truth)

def main(args):
    data_type_code, data_type_size = pc.get_data_type_code(args.data_type)
    base_data = pc.DataMat(data_type_code, data_type_size)
    base_data.load_bin(args.base_file)

    query_data = pc.DataMat(data_type_code, data_type_size)
    query_data.load_bin(args.query_file)

    print("Grouping base vectors by filters\n")
    base_filters, base_filtered_vectors = process_filters(args.base_filter_file, base_data, is_query=False)
    print("Grouping query vectors by filters\n")
    query_filters, query_filtered_vectors = process_filters(args.query_filter_file, query_data, is_query=True)

    compute_gt(base_filtered_vectors, query_filtered_vectors, args.dist_fn)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Generate GTs for a given base and query file and compare with any existing GT file. Assumes that there is a ton of memory to process the data!!!', prog='GTVerifier.py')
    parser.add_argument('--base_file', type=str, help='Base file', required=True)
    parser.add_argument('--query_file', type=str, help='Query file', required=True)
    parser.add_argument('--base_filter_file', type=str, help='Base filter file', required=False)
    parser.add_argument('--query_filter_file', type=str, help='Query filter file', required=False)
    parser.add_argument('--output_gt_file', type=str, help='Output GT file', required=True)
    parser.add_argument('--existing_gt_file', type=str, help='Existing GT file', required=False)
    parser.add_argument('--dist_fn', type=str, help='GT format', required=True)
    parser.add_argument('--data_type', type=str, help='GT format', required=True)
    args = parser.parse_args()


    main(args)


